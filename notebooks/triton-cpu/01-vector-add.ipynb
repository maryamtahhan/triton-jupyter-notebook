{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Addition\n",
    "\n",
    "In this tutorial, you will write a simple vector addition using Triton.<br>\n",
    "In doing so, you will learn about:<br>\n",
    "* The basic programming model of Triton.<br>\n",
    "* The `triton.jit` decorator, which is used to define Triton kernels.<br>\n",
    "* The best practices for validating and benchmarking your custom ops against native reference implementations.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_BLOCK_SIZE = 1024\n",
    "CPU_BLOCK_SIZE = 4096\n",
    "# Single Thread Threshold\n",
    "CPU_ST_THRESHOLD = 65536\n",
    "USE_GPU = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def add_kernel(\n",
    "    x_ptr,  # *Pointer* to first input vector.\n",
    "    y_ptr,  # *Pointer* to second input vector.\n",
    "    output_ptr,  # *Pointer* to output vector.\n",
    "    n_elements,  # Size of the vector.\n",
    "    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.\n",
    "    # NOTE: `constexpr` so it can be used as a shape value.\n",
    "):\n",
    "    # There are multiple 'programs' processing different data. We identify which program\n",
    "    # we are here:\n",
    "    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.\n",
    "    # This program will process inputs that are offset from the initial data.\n",
    "    # For instance, if you had a vector of length 256 and block_size of 64, the programs\n",
    "    # would each access the elements [0:64, 64:128, 128:192, 192:256].\n",
    "    # Note that offsets is a list of pointers:\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    # Create a mask to guard memory operations against out-of-bounds accesses.\n",
    "    mask = offsets < n_elements\n",
    "    # Load x and y from DRAM, masking out any extra elements in case the input is not a\n",
    "    # multiple of the block size.\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    y = tl.load(y_ptr + offsets, mask=mask)\n",
    "    output = x + y\n",
    "    # Write x + y back to DRAM.\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def add_kernel_tiled(\n",
    "    x_ptr,  # *Pointer* to first input vector.\n",
    "    y_ptr,  # *Pointer* to second input vector.\n",
    "    output_ptr,  # *Pointer* to output vector.\n",
    "    n_elements,  # Size of the vector.\n",
    "    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.\n",
    "    TILE_SIZE: tl.constexpr,  # Number of elements each iteration should process.\n",
    "    # NOTE `constexpr` so it can be used as a shape value.\n",
    "):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    for i in range(0, tl.cdiv(BLOCK_SIZE, TILE_SIZE)):\n",
    "        offsets = block_start + i * TILE_SIZE + tl.arange(0, TILE_SIZE)\n",
    "        mask = offsets < n_elements\n",
    "        x = tl.load(x_ptr + offsets, mask=mask)\n",
    "        y = tl.load(y_ptr + offsets, mask=mask)\n",
    "        output = x + y\n",
    "        tl.store(output_ptr + offsets, output, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "    configs=[\n",
    "        # For small vectors it might be faster to use a single thread instead\n",
    "        # of paying OMP threading overhead, so add a single-threaded option.\n",
    "        # Other options use all available threads.\n",
    "        triton.Config({\"TILE_SIZE\": 16, \"BLOCK_SIZE\": 4096}, num_threads=1),\n",
    "        triton.Config({\"TILE_SIZE\": 16, \"BLOCK_SIZE\": 4096}, num_threads=0),\n",
    "        triton.Config({\"TILE_SIZE\": 16, \"BLOCK_SIZE\": 8192}, num_threads=0),\n",
    "        triton.Config({\"TILE_SIZE\": 16, \"BLOCK_SIZE\": 16384}, num_threads=0),\n",
    "        triton.Config({\"TILE_SIZE\": 16, \"BLOCK_SIZE\": 32768}, num_threads=0),\n",
    "        triton.Config({\"TILE_SIZE\": 16, \"BLOCK_SIZE\": 65536}, num_threads=0),\n",
    "    ],\n",
    "    key=[\"n_elements\"],\n",
    ")\n",
    "@triton.jit\n",
    "def add_kernel_tiled_autotuned(\n",
    "    x_ptr,  # *Pointer* to first input vector.\n",
    "    y_ptr,  # *Pointer* to second input vector.\n",
    "    output_ptr,  # *Pointer* to output vector.\n",
    "    n_elements,  # Size of the vector.\n",
    "    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.\n",
    "    TILE_SIZE: tl.constexpr,  # Number of elements each iteration should process.\n",
    "):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    for i in range(0, tl.cdiv(BLOCK_SIZE, TILE_SIZE)):\n",
    "        offsets = block_start + i * TILE_SIZE + tl.arange(0, TILE_SIZE)\n",
    "        mask = offsets < n_elements\n",
    "        x = tl.load(x_ptr + offsets, mask=mask)\n",
    "        y = tl.load(y_ptr + offsets, mask=mask)\n",
    "        output = x + y\n",
    "        tl.store(output_ptr + offsets, output, mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also declare a helper function to (1) allocate the `z` tensor\n",
    "and (2) enqueue the above kernel with appropriate grid/block sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(x: torch.Tensor, y: torch.Tensor, output: torch.Tensor, device):\n",
    "    if output is None:\n",
    "        # We need to preallocate the output.\n",
    "        output = torch.empty_like(x)\n",
    "    n_elements = output.numel()\n",
    "    # The SPMD launch grid denotes the number of kernel instances that run in parallel.\n",
    "    # It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int].\n",
    "    # In this case, we use a 1D grid where the size is the number of blocks:\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n",
    "    # NOTE:\n",
    "    #  - Each torch.tensor object is implicitly converted into a pointer to its first element.\n",
    "    #  - `triton.jit`'ed functions can be indexed with a launch grid to obtain a callable GPU kernel.\n",
    "    #  - Don't forget to pass meta-parameters as keywords arguments.\n",
    "    add_kernel[grid](\n",
    "        x,\n",
    "        y,\n",
    "        output,\n",
    "        n_elements,\n",
    "        BLOCK_SIZE=CPU_BLOCK_SIZE if device == \"cpu\" else GPU_BLOCK_SIZE,\n",
    "    )\n",
    "    # We return a handle to z but, since `torch.cuda.synchronize()` hasn't been called, the kernel is still\n",
    "    # running asynchronously at this point.\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tiled(x: torch.Tensor, y: torch.Tensor, output):\n",
    "    if output is None:\n",
    "        output = torch.empty_like(x)\n",
    "    n_elements = output.numel()\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n",
    "    add_kernel_tiled[grid](\n",
    "        x, y, output, n_elements, BLOCK_SIZE=CPU_BLOCK_SIZE, TILE_SIZE=16\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tiled_with_st_threshold(x: torch.Tensor, y: torch.Tensor, output):\n",
    "    if output is None:\n",
    "        output = torch.empty_like(x)\n",
    "    n_elements = output.numel()\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n",
    "    # TODO: try to choose the best block size using autotuner\n",
    "    BLOCK_SIZE = triton.next_power_of_2(n_elements)\n",
    "    if BLOCK_SIZE > CPU_ST_THRESHOLD:\n",
    "        BLOCK_SIZE = CPU_BLOCK_SIZE\n",
    "    add_kernel_tiled[grid](\n",
    "        x, y, output, n_elements, BLOCK_SIZE=BLOCK_SIZE, TILE_SIZE=16\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tiled_autotuned(x: torch.Tensor, y: torch.Tensor, output):\n",
    "    if output is None:\n",
    "        output = torch.empty_like(x)\n",
    "    n_elements = output.numel()\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n",
    "    add_kernel_tiled_autotuned[grid](x, y, output, n_elements)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the above function to compute the element-wise sum of two `torch.tensor` objects and test its correctness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "size = 98432"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triton.runtime.driver.set_active_to_cpu()\n",
    "x = torch.rand(size, device=\"cpu\")\n",
    "y = torch.rand(size, device=\"cpu\")\n",
    "output_torch_cpu = torch.add(x, y)\n",
    "output_triton_cpu = add(x, y, None, device=\"cpu\")\n",
    "print(output_torch_cpu)\n",
    "print(output_triton_cpu)\n",
    "print(\n",
    "    f\"The maximum difference between torch-cpu and triton-cpu is \"\n",
    "    f\"{torch.max(torch.abs(output_torch_cpu - output_triton_cpu))}\"\n",
    ")\n",
    "output_triton_cpu = add_tiled(x, y, None)\n",
    "print(\n",
    "    f\"The maximum difference between torch-cpu-tiled and triton-cpu is \"\n",
    "    f\"{torch.max(torch.abs(output_torch_cpu - output_triton_cpu))}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINE_VALS = [\n",
    "    \"triton-cpu\",\n",
    "    \"triton-cpu-hooks\",\n",
    "    \"triton-cpu-tiled\",\n",
    "    \"triton-cpu-tiled-hooks\",\n",
    "    \"triton-cpu-tiled-tuned-hooks\",\n",
    "    \"triton-cpu-tiled-autotuned-hooks\",\n",
    "    \"torch-cpu\",\n",
    "]\n",
    "LINE_NAMES = [\n",
    "    \"TritonCPU\",\n",
    "    \"TritonCPU (hooks)\",\n",
    "    \"TritonCPUTiled\",\n",
    "    \"TritonCPUTiled (hooks)\",\n",
    "    \"TritonCPUTiled (tuned, hooks)\",\n",
    "    \"TritonCPUTiled (autotuned, hooks)\",\n",
    "    \"TorchCPU\",\n",
    "]\n",
    "LINE_STYLES = [\n",
    "    (\"blue\", \"--\"),\n",
    "    (\"blue\", \"-.\"),\n",
    "    (\"red\", \"-\"),\n",
    "    (\"red\", \"--\"),\n",
    "    (\"red\", \"-.\"),\n",
    "    (\"red\", \":\"),\n",
    "    (\"green\", \"-\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_GPU and triton.runtime.driver.get_active_gpus():\n",
    "    triton.runtime.driver.set_active_to_gpu()\n",
    "    x = x.to(\"cuda\")\n",
    "    y = y.to(\"cuda\")\n",
    "    output_torch_gpu = x + y\n",
    "    output_triton_gpu = add(x, y, None, device=\"cuda\")\n",
    "    print(output_torch_gpu)\n",
    "    print(output_triton_gpu)\n",
    "    print(\n",
    "        f\"The maximum difference between torch-gpu and triton-gpu is \"\n",
    "        f\"{torch.max(torch.abs(output_torch_gpu - output_triton_gpu))}\"\n",
    "    )\n",
    "    LINE_VALS += [\"triton-gpu\", \"torch-gpu\"]\n",
    "    LINE_NAMES += [\"TritonGPU\", \"TorchGPU\"]\n",
    "    LINE_STYLES += [(\"yellow\", \"-\"), (\"red\", \"-\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like we're good to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark\n",
    "We can now benchmark our custom op on vectors of increasing sizes to get a sense of how it does relative to PyTorch.\n",
    "To make things easier, Triton has a set of built-in utilities that allow us to concisely plot the performance of our custom ops.\n",
    "for different problem sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=[\"size\"],  # Argument names to use as an x-axis for the plot.\n",
    "        x_vals=[\n",
    "            2**i for i in range(12, 28, 1)\n",
    "        ],  # Different possible values for `x_name`.\n",
    "        x_log=True,  # x axis is logarithmic.\n",
    "        line_arg=\"provider\",  # Argument name whose value corresponds to a different line in the plot.\n",
    "        line_vals=LINE_VALS,  # Possible values for `line_arg`.\n",
    "        line_names=LINE_NAMES,  # Label name for the lines.\n",
    "        styles=LINE_STYLES,  # Line styles.\n",
    "        ylabel=\"GB/s\",  # Label name for the y-axis.\n",
    "        plot_name=\n",
    "        # Name for the plot. Used also as a file name for saving the plot.\n",
    "        f\"vector-add-performance (CPU_BLOCK_SIZE={CPU_BLOCK_SIZE}, GPU_BLOCK_SIZE={GPU_BLOCK_SIZE})\",\n",
    "        args={},  # Values for function arguments not in `x_names` and `y_name`.\n",
    "    )\n",
    ")\n",
    "def benchmark(size, provider):\n",
    "    device = \"cpu\" if \"cpu\" in provider else \"cuda\"\n",
    "    x = torch.rand(size, device=device, dtype=torch.float32)\n",
    "    y = torch.rand(size, device=device, dtype=torch.float32)\n",
    "    if device == \"cpu\":\n",
    "        triton.runtime.driver.set_active_to_cpu()\n",
    "    else:\n",
    "        triton.runtime.driver.set_active_to_gpu()\n",
    "    output = torch.empty_like(x)\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == \"torch-gpu\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: x + y, quantiles=quantiles)\n",
    "    elif provider == \"triton-gpu\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: add(x, y, None, False), quantiles=quantiles\n",
    "        )\n",
    "    elif provider == \"torch-cpu\":\n",
    "        # Note that we preallocate the output buffer here to only measure the kernel performance\n",
    "        # without a large chunk of memory allocation.\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: torch.add(x, y, out=output), quantiles=quantiles\n",
    "        )\n",
    "    elif provider == \"triton-cpu\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: add(x, y, output, device), quantiles=quantiles\n",
    "        )\n",
    "    elif provider == \"triton-cpu-hooks\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: add(x, y, output, device),\n",
    "            quantiles=quantiles,\n",
    "            measure_time_with_hooks=True,\n",
    "        )\n",
    "    elif provider == \"triton-cpu-tiled\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: add_tiled(x, y, output), quantiles=quantiles\n",
    "        )\n",
    "    elif provider == \"triton-cpu-tiled-hooks\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: add_tiled(x, y, output),\n",
    "            quantiles=quantiles,\n",
    "            measure_time_with_hooks=True,\n",
    "        )\n",
    "    elif provider == \"triton-cpu-tiled-tuned-hooks\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: add_tiled_with_st_threshold(x, y, output),\n",
    "            quantiles=quantiles,\n",
    "            measure_time_with_hooks=True,\n",
    "        )\n",
    "    elif provider == \"triton-cpu-tiled-autotuned-hooks\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: add_tiled_autotuned(x, y, output),\n",
    "            quantiles=quantiles,\n",
    "            measure_time_with_hooks=True,\n",
    "        )\n",
    "    gbps = lambda ms: 3 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "    return gbps(ms), gbps(max_ms), gbps(min_ms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the decorated function above. Pass `print_data=True` to see the performance number, `show_plots=True` to plot them, and/or\n",
    "`save_path='/path/to/results/' to save them to disk along with raw CSV data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark.run(print_data=True, show_plots=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
